{
    "model_type": "split",
    "axiom_order": "length",
    "attention": "none",
    "stateful": true,
    "normalize": false,
    "batch_norm": false,
    "rnn_type": "lstm",
    "no_rnn_units": 32,
    "embedding_size": 50,
    "no_dense_units": 512,
    "dropout_rate": 0.1,
    "learning_rate": 0.001,
    "target_vocab_size": 6000,
    "global_max_pool": false,
    "input_vocab_size": 4000,
    "remove_unknown": true,
    "teacher_forcing_rate": 1.0,
    "transformer_num_layers": 2,
    "transformer_dense_units": 128,
    "num_attention_heads": 2,
    "encoder_type": "transformer",
    "decoder_type": "transformer",
    "conjecture_input_length": 200
}
